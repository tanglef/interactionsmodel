\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Convergence rates}\label{chap:an_cv}

As a stopping criterion, we choose the distance from $0$ to the
subdifferential of the primal objective function taken at the current iterate.
Convergence rates can be explicited as it is often done for the objectives
convergence.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A simple case: Ordinary-Least-Squares}
\label{sec:a_simple_case_ordinary_least_squares}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let us start by considering a differentiable case.
In particular, we investigate the least-squares case:
\begin{align*}
    \cP_{OLS}(\beta) = \frac{1}{2n}\norm{X\beta - y}^2_2 \enspace.
\end{align*}
We aim at upper bounding
\begin{align}\label{eq:diff_dist}
    \Delta\left(\beta^k, \beta^*\right)
    =
    d_{\norm{\cdot}_\infty}\left(0, \partial\cP_{OLS}(\beta^k)\right) -
    d_{\norm{\cdot}_\infty}\left(0, \partial \cP_{OLS}(\beta^{*\!\!\!\phantom{k}})\right) \enspace,
\end{align}
where $\beta^k$ is the $k^{th}$ iterate generated by gradient descent
and $\beta^*$ is a minimizer\footnote{we assume it to be unique for simplicity.}.
Since $\beta^*$ is a minimizer, we can simplify \Cref{eq:diff_dist}.
The subdifferential $\partial\cP_{OLS}(\beta^*)$ reduces to the singleton
$\{\nabla\cP_{OLS}(\beta^*)\}=\{0\}$ by optimality.
Hence, to upper bound \Cref{eq:diff_dist} one can simply upper bound
$d(0, \nabla \cP_{OLS}(\beta^k))=\norm{\nabla \cP_{OLS}(\beta^k)}_\infty$.

\begin{proposition}\label{prop:ls_kkt}
    For $X\in\bbR^{n\times p}$, $k\geq 0$, $\gamma = \nicefrac{1}{L}$
    with $L$ (resp. $\mu$) the largest (resp. smallest) eigenvalue of
    the Hessian of $\cP_{OLS}$ at optimum, and $\kappa=\nicefrac{L}{\mu}$ the condition
    number, we have the following inequality:
    \begin{align}
    \norm{\partial\cP_{OLS}(\beta^k)}_{\infty}
    \leq pL \exp\left(-\frac{k}{\kappa}\right) \norm{\beta^0 - \beta^*}_\infty
    \enspace.
    \end{align}
\end{proposition}
\begin{proof}
We follow here the line of the proof showing the convergence of the objectives by \citep{Bachbook}.
Remark that the Hessian matrix ($\frac{\partial^2{\cP_{OLS}}}{\partial^2 \beta}(\cdot)$)
is constant for all $\beta\in \bbR^p$, so we write $H$ for this matrix.
In particular
\begin{align}
    H & = \frac{1}{n}X^\top X\enspace, \\
    H\beta^* & = \frac{1}{n}X^\top y \enspace.
\end{align}
It follows that
\begin{align}\label{eq:grad_ols}
\nabla \cP_{OLS}(\beta^k) = H(\beta^k - \beta^*) \enspace.
\end{align}
We can iterate the gradient descent update formula (\Cref{eq:gd_step}) with step size $\gamma$,
leading to:
\begin{align*}
    \nabla \cP_{OLS}(\beta^k)
     & = H\big(\beta^{k-1} - \gamma \nabla \cP(\beta^{k-1}) - \beta^*\big) \\
     & \stackrel{\mathmakebox[\widthof{=}]{{\ref{eq:grad_ols}}}}{=}  H(\Id - \gamma H) (\beta^{k-1} - \beta^*) \\
     & = H(\Id - \gamma H)^k (\beta^0 - \beta^*) \enspace.
\end{align*}
%
Taking the infinite norm on both sides, we get:
\begin{align*}
    \norm{\nabla \cP_{OLS}(\beta^k)}_\infty
    & = \norm{H(\Id - \gamma H)^k (\beta^0 - \beta^*)}_\infty \\
    & \leq \tnorm{H}_\infty \tnorm{(\Id - \gamma H)^k}_\infty \norm{\beta^0 - \beta^*}_\infty \enspace,
\end{align*}
where for a matrix $M\in\bbR^{n\times p}$, $\tnorm{M}_\infty$ is the induced norm
of $\norm{\cdot}_\infty$ applied to $M$, defined as:
\begin{align*}
\tnorm{M}_\infty = \max_{j\in [p]} \sum_{i=1}^n \abs{m_{ij}} \enspace.
\end{align*}
Using that $\tnorm{M}_\infty \leq \sqrt{p} \tnorm{M}_2$,
\begin{align*}
    \norm{\nabla \cP_{OLS}(\beta^k)}_\infty
    & \leq \sqrt{p}\tnorm{H}_2 \sqrt{p}\tnorm{(\Id - \gamma H)^k}_2
           \norm{\beta^0 - \beta^*}_\infty
    \enspace.
\end{align*}
Finally, if we denote $\eta\in\bbR_+^*$ an eigenvalue of the Hessian $H$, then
(using the spectral theorem),
for a continuous function $\varphi$, $\varphi(\eta)$ is an eigenvalue of
$\varphi(H)$. We can use this property to find an upper bound of
$\tnorm{(\Id - \gamma H)^k}_2$:
\begin{align*}
    \max_{\eta \in [\mu, L]}(1-\gamma \eta)^k
    & = \left(1-\frac{\mu}{L}\right)^k \\
    & = \left(1- \kappa^{-1}\right)^k
    \enspace.
\end{align*}

Combining our results leads to \Cref{prop:ls_kkt}:
\begin{align*}
\norm{\partial\cP_{OLS}(\beta^k)}_{\infty}
& \leq p L \left(1- \kappa^{-1}\right)^k \norm{w^0 - w^*}_\infty \\
& \leq pL \exp\left(-\frac{k}{\kappa}\right) \norm{\beta^0 - \beta^*}_\infty
\enspace.
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ridge regularization}
\label{sec:ridge_regularization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Building up to the Elastic-Net, let us consider the Ridge regularization with
$\lambda\geq 0$ as $\ell_2$ penalty.
The primal considered is:
\begin{align}
    \cP_{Ridge} = \frac{1}{2n}\norm{X\beta-y}_2^2 + \frac{\lambda}{2}\norm{\beta}_2^2
    \enspace.
\end{align}

\begin{proposition}\label{prop:ridge_kkt}
    For $X\in\bbR^{n\times p}$, $k\geq 0$, $\gamma = \nicefrac{1}{L}$
    with $L$ (resp. $\mu$) the largest (resp. smallest) eigenvalue of
    the Hessian of $\cP$ at optimum, and $\kappa=\nicefrac{\mu}{L}$ the condition
    number of $X$, we have the following inequality:
    \begin{align}
    \norm{\partial\cP_{Ridge}(\beta^k)}_{\infty}
    \leq pL \exp\left(-\frac{k}{\kappa}\right) \norm{\beta^0 - \beta^*}_\infty
    \enspace.
    \end{align}
\end{proposition}
Meaning that the convergence rate for Ridge regularization and Ordinary-Least-Squares
only differs by the condition number.

\begin{proof}
    The first order conditions give us:
    \begin{align*}
        \nabla \cP_{Ridge}(\beta)
        & = \frac{1}{n}X^\top (X\beta-y)+\lambda \beta \\
        & = \left[\frac{1}{n}X^\top X + \lambda \Id\right]\beta - \frac{1}{n}X^\top y \\
        & = \left[\frac{1}{n}X^\top X + \lambda \Id\right](\beta - \beta^*) \enspace.
    \end{align*}
    Noticing that $\frac{1}{n}X^\top X + \lambda \Id$ is the Hessian matrix of
    $\cP_{Ridge}$, constant for all $\beta\in\bbR^p$, we
    thus recover:
    \begin{align} \label{eq:same_ols}
        \nabla \cP_{Ridge}(\beta) = H(\beta - \beta^*)\enspace.
    \end{align}
    \Cref{eq:same_ols} is exactly the same as the OLS case. Thus with
    the exact same steps we obtain \Cref{prop:ridge_kkt}.
\end{proof}
A visualization of the convergence rate in \Cref{prop:ridge_kkt} is
available in \Cref{fig:importance_stepsize}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LASSO regularization}
\label{sec:LASSO_regularization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \Cref{sec:a_simple_case_ordinary_least_squares} and \Cref{sec:ridge_regularization}
we used the closed form of the solution. With the LASSO, we do not have this.
Let us consider the primal with $\lambda>0$ as penalty:
\begin{align}\label{eq:primal_lasso}
\cP_{LASSO}(\beta) = \frac{1}{2n} \norm{X\beta - y}_2 ^2 + \lambda \norm{\beta}_1
\enspace.
\end{align}

\begin{proposition}\label{prop:lasso_kkt}
Under the assumptions of \Cref{prop:ls_kkt}:
\begin{align}
    \Delta(\beta^k, \beta^*)
    \leq pL \exp\left(-\frac{k}{\kappa}\right) \norm{\beta^0 - \beta^*}_\infty
    \enspace.
\end{align}
\end{proposition}
\begin{proof}
The subgradient of $\cP_{LASSO}$ at iterate $k\geq 0$ writes:
\begin{align*}
\partial \cP_{LASSO} (\beta^k)
= \frac{1}{n}X^\top (X\beta^k - y)
+ \lambda \partial_{\norm{\cdot}_1}(\beta^k) \\
\enspace.
\end{align*}
Then
\begin{align}
\Delta(\beta^k, \beta^*)
& = d\left(\frac{1}{n}X^\top (y-X\beta^k),
 \lambda \partial_{\norm{\cdot}_1}(\beta^k)  \right)\nonumber \\
& = \norm{\ST\left(\frac{1}{n}X^\top (y-X\beta^k), \lambda\right)}_\infty \nonumber \\
& \leq \norm{\frac{1}{n}X^\top (y-X\beta^k)}_\infty \label{ineq:st_non_expansive}\\
& \leq \norm{\nabla \cP_{OLS}(\beta^k)}_\infty
\enspace,\nonumber
\end{align}
with \Cref{ineq:st_non_expansive} obtained using that the soft-thresholding operator
is non-expansive coordinate-wise \citep[Lemma 3.2]{hale2008fixed} \ie
\begin{align*}
\forall x,y\in\bbR^n\ \forall i\in [n],\ \abs{\ST(x, \lambda)_i - \ST(y, \lambda)_i}
\leq \abs{x_i - y_i}\enspace.
\end{align*}
Using \Cref{prop:ls_kkt} we obtain our upper bound.
\end{proof}

Notice that \Cref{ineq:st_non_expansive} is in fact not threshold-dependant.
So in a weighted LASSO problem with penalties $(\lambda_j)_{j=1}^p$, we would have:
\begin{align*}
\Delta(\beta^k, \beta^*)
& = \max_{j\in[p]}\abs{\ST\left(\frac{1}{n}X^\top_j (y-X\beta^k), \lambda_j\right)} \\
& \leq \max_{j\in[p]}\abs{\frac{1}{n}X^\top_j (y-X\beta^k)} \\
& \leq \norm{\nabla \cP_{OLS}(\beta^k)}_\infty
\enspace.
\end{align*}
Thus leading to the same upper bound.

\subsection{Elastic-Net regularization}

For the Elastic-Net, we consider the primal:
\begin{align}
\cP_{Enet}(\beta) = \frac{1}{2n}\norm{X\beta - y}_2^2
+ \lambda_{\ell_1}\norm{\beta}_1
+ \frac{\lambda_{\ell_2}}{2}\norm{\beta}_2^2
\enspace.
\end{align}

The Elastic-Net being an augmented LASSO problem (\Cref{sub:equivLasso}), we have
the following corollary.

\begin{corollary}\label{cor:enet_kkt}
For $\widetilde y=[y\,|\, 0_p]^\top$,
$\widetilde X =
\begin{bmatrix}
     X \\ \sqrt{\lambda_{\ell_2} n}\mathrm{Id}_{p\times p}\\
\end{bmatrix}$, we get:
\begin{align*}
\Delta(\beta^k, \beta^*)
\leq p L_{\widetilde X}
\exp\left(-\frac{k}{\kappa_{\widetilde{X}}}\right) \norm{\beta^0 - \beta^*}_\infty
\enspace,
\end{align*}
with $L_{\widetilde X} = \frac{\sigma_{\max}(X) + n\lambda_{\ell_2}}{n}$
and $\kappa_{\widetilde{X}}
= \frac{\sigma_{\max}(X) + n\lambda_{\ell_2}}{\sigma_{\min}(X) + n\lambda_{\ell_2}}.$
\end{corollary}
\begin{proof}
We only need to notice that the spectrum of $\widetilde X^\top \widetilde X$ is the
same as $X^\top X$ shifted of $n\lambda_{\ell_2}$. Then we apply \Cref{prop:lasso_kkt}
to \Cref{eq:primal_lasso} with $\widetilde y$ and $\widetilde X$.
\end{proof}
We can visualize the impact of the $\ell_2$ regularization on the convergence.
\Cref{fig:cv_rate_enet} shows that with larger $\ell_2$ regularizations, the
proximal descent for the Elastic-Net converges faster.
This is a visualization of the dependance of $\Delta$ by the condition number
from \Cref{cor:enet_kkt}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=.7]{cv_rate_enet}
    \caption{Elastic-Net with $\ell_1$ penalty set to $0.5$ and varying $\ell_2$
    penalties on the leukemia dataset.
    For information, $\lambda_{\max}\simeq 0.626$ for this dataset.
    }
    \label{fig:cv_rate_enet}
\end{figure}


\end{document}
